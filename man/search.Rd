% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/search.R
\name{search}
\alias{search}
\title{Identify local and global optima.}
\usage{
search(f, npar, minimize = TRUE, controls = NULL, quiet = TRUE, seed = NULL)
}
\arguments{
\item{f}{A function that computes value, gradient, and Hessian of the function to be
optimized and returns them as a named list with elements \code{value},
\code{gradient}, and \code{hessian}. It is required that \code{hessian} is
of class \code{matrix}.}

\item{npar}{The number of parameters of \code{f}.}

\item{minimize}{A boolean, determining whether \code{f} should be minimized
\code{minimize = TRUE} (the default) or maximized \code{minimize = FALSE}.}

\item{controls}{Either \code{NULL} or a named list with the following elements. Missing
elements are set to default values (printed in parentheses).
\itemize{
\item \code{init_runs} \code{(5)}:
The number of short initial searches.
\item \code{init_min} \code{(-1)}:
The minimum value for the random initialization.
\item \code{init_max} \code{(1)}:
The maximum value for the random initialization.
\item \code{init_iterlim} \code{(20)}:
The number of iterations for the short initial searches.
\item \code{neighborhoods} \code{(5)}:
The number of nested neighborhoods.
\item \code{neighbors} \code{(5)}:
The number of neighbors in each neighborhood.
\item \code{beta} \code{(0.05)}:
A non-negative weight factor to account for the function's curvature in the
selection of the neighbors. If \code{beta = 0}, the curvature is ignored.
The higher the value, the higher probability of selecting a neighbor in the
direction of the highest function curvature.
\item \code{iterlim} \code{(1000)}:
The maximum number of iterations to be performed before the local search is
terminated.
\item \code{only_global} \code{(FALSE)}:
A boolean. Set \code{only_global = TRUE} if you are only interested in the
global optimum. In this case, the algorithm will interrupt any local search
prematurely that converges to a local optimum. This saves computation time.
If \code{only_global = FALSE} (the default), the algorithm also looks for local
optima.
\item \code{tolerance} \code{(1e-5)}:
Passed on to \code{\link{all.equal}}, which is used to check if two optima
are equal.
}}

\item{quiet}{Set \code{quiet = FALSE} to print progress.}

\item{seed}{Set a seed for the sampling of the random starting points.}
}
\value{
A list of two list,
\itemize{
\item \code{global} optima and
\item \code{local} optima,
}
each of which contains lists with
\itemize{
\item \code{value} and
\item \code{argument}
}
of each identified optimum.
}
\description{
Function that identifies local and global optima via variable neighborhood
trust region search (VNTRS).
}
\examples{
### Gramacy & Lee function
gramacy_lee = function(x) {
 stopifnot(is.numeric(x))
 stopifnot(length(x) == 1)
 f = expression(sin(10*pi*x) / (2*x) + (x-1)^4)
 g = D(f, "x")
 h = D(g, "x")
 f = eval(f)
 g = eval(g)
 h = eval(h)
 list(value = f, gradient = g, hessian = as.matrix(h))
}
search(f = gramacy_lee, npar = 1, seed = 1)

### Shubert function
shubert = function(x) {
 stopifnot(is.numeric(x))
 stopifnot(length(x) == 2)
 f = expression(
   (cos(2*x1+1) + 2*cos(3*x1+2) + 3*cos(4*x1+3) + 4*cos(5*x1+4) +
   5*cos(6*x1+5)) *
   (cos(2*x2+1) + 2*cos(3*x2+2) + 3*cos(4*x2+3) + 4*cos(5*x2+4) +
   5*cos(6*x2+5))
 )
 g1 = D(f, "x1")
 g2 = D(f, "x2")
 h11 = D(g1, "x1")
 h12 = D(g1, "x2")
 h22 = D(g2, "x2")
 x1 = x[1]
 x2 = x[2]
 f = eval(f)
 g = c(eval(g1), eval(g2))
 h = rbind(c(eval(h11), eval(h12)), c(eval(h12), eval(h22)))
 list(value = f, gradient = g, hessian = h)
}
search(f = shubert, npar = 2, seed = 1)

### Rosenbrock function
rosenbrock = function(x) {
  stopifnot(is.numeric(x))
  stopifnot(length(x) == 2)
  f = expression(100 * (x2 - x1^2)^2 + (1 - x1)^2)
  g1 = D(f, "x1")
  g2 = D(f, "x2")
  h11 = D(g1, "x1")
  h12 = D(g1, "x2")
  h22 = D(g2, "x2")
  x1 = x[1]
  x2 = x[2]
  f = eval(f)
  g = c(eval(g1), eval(g2))
  h = rbind(c(eval(h11), eval(h12)), c(eval(h12), eval(h22)))
  list(value = f, gradient = g, hessian = h)
}
search(f = rosenbrock, npar = 2, seed = 1)
}
\references{
Bierlaire et al. (2009) "A Heuristic for Nonlinear Global Optimization"
\url{https://doi.org/10.1287/ijoc.1090.0343}
}
